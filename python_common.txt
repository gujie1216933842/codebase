查看python版本
python
import  django
django get_version()



创建一个项目(不在python脚本)
django-admin 需要配置环境变量
django-admin  startproject  mysite
         - mysite
            - mysite
                - init
                - wsgi    规则,接口,遵循wsgi防范
          - manage.py     管理django程序
                     python manage.py    
                     python manage.py  startapp  xx     (在manage.py文件同级下创建)
             
              
             (之前需要则setting中注册应用) INSTALLED_APPS列表中添加应用名  
             python manage.py  makemigrations   goods 写好后生成迁移(本质生成sql语句)(以goodsapp为例) 
                     python manage.py  migrate    执行迁移(本质执行sql语句)

django创建应用提示没有mysqldb模块而报错
解决方法:
在项目的setting.py文件头部添加下面两端代码(py2中的mysqldb相当于py3中的pymysql)
import pymysql
pymysql.install_as_MySQLdb()

在setting.py文件中installed_apps列表中添加配置
执行命令  安装: python manage.py makemigrations   生成一个py文件
          迁移: python manage.py migrate


python manage.py runserver

{%csrf_token%} /*为了防止表单提交报403错误*/


用pycharm也能创建django程序    file/new project      
           

app文件夹下自建文件的作用
      migreation   数据修改表结构
      admin    django为我们提供的后台管理
      apps     配置当前的app
      models   orm,写指定的类  通过命令可以创建数据库结构
      tests    单元测试
      views    业务代码                      

主应用文件下没有view.py文件   (主应用文件夹名和django项目名一致)


django 创建后台操作
python manage.py createsuperuser
如果需要在admin后台引入想要设置的模型
现在项目各自应用文件夹下添加文件 admin.py   以项目为例 : tiantian/shopping/goods/下添加admin.py文件






    


进入mysite(不在python脚本内)  
python  manage.py  runserver  127.0.0.1:8001    (注:端口号可自己定义)

在阿里云服务器上启动django
python  manage.py  runserver  0.0.0.0:9000   (端口号可以自己定义,但是必须在阿里云服务器后台安全组添加端口权限)




创建完项目之后,配置模板的路径,配置静态目录
   静态文件:setting.py下
               STATICFIFLES_DIRS=(
                     os.path.join(BASE_DIR,'static'),
)


再在模板文件中
                <link rel='stylesheet' href='/static/commons.css'>



csrf先注释掉     settings.py中middleware中找到



django 模板语言中for循环和if
{ % for  item in items %}
   {{ item.name }}
   {{ item.age }}
{ % endfor %}

条件判断 {% if 条件 %}{% else %}{% endif %}


django请求生命周期
模板页面向后台发送请求url的字符串,后台服务器中已经存在的对应关系,一个str对应一个函数,如果匹配成功,就执行相应的方法,并且通过open函数打开html文件,如果需要传递啥参数的就需要字符串替换,然后response返回一个html的字符串,返回给用户(渲染过程)

mysite
    mysite
       - 配置文件
       - url.py
       - setting.py
    cmdb
       - view.py
       - admin.py
       - models.py     #模型层,创建数据库表
   





def  func(request):
    #包含请求数据
     return  HttpResponse('字符串')
     return  render(request ,'index.html',{'':""})  #带字典传参数
     return  render(request ,'index.html')  #不带参数展示模板
     return  redirect('url')    #重定向,跳转其他页面


views.py中获取前端传入后台的内容
     request.GET.get('username',None)  此方法更保险,不报错,get()是字典中取数据的方法
     request.POST['username']
     request.FILES

     #checkbox等多选的内容
     obj = request.POST.getlist()

     #上传文件
     obj = open(obj.name , mode = 'wb')
     for  item in obj.chunks():
               f.write(item)
     f.close()


FBV和CBV
fbv:  function base view
    url.py
        index ->函数名
    view.py
        def 函数(request):
             ..... 
cbv:  class base view
     
      

面向对象中,如果子类中的方法名和父类中的方法名相同,
如果不做处理,只会执行子类中的方法,
如果既想执行子类中的方法,也想执行父类中的方法,需要写
(以dispatch为例)
super(Home,self).dispatch(.......)




  模块:用来从逻辑上组织python代码 ,本质上是以py结尾的文件
  包: 本质就是一个文件夹,目录(区别就是必须带有一个__init__.py文件) 
      包是用来从逻辑上组织模块的,里面存放的一堆python文件
      导入包本质就是执行包下的__init__.py文件 
  导入方法:  
     import  module1_name  ,module2_name

  import本质:







URLError:(产生的原因)
1:连不上服务器
2:远程的url不存在
3:加入本地没网络
4:假如促发子类HTTPError


表单提交网页http://www.iqianyue.com/mypost/


scrapy框架有两种命令
全局命令和项目命令    项目里面自然也可以使用全局命令
遇到不懂的命令   scrapy  命令  -h   显示该命令的使用方式
shell命令进入scrapy的命令终端
view 下载某个网页,并且用浏览器查看
     




scrapy startproject  first  创建项目


新建爬虫文件


正则表达式
xpath表达式    运行速度比正则表达式要快,基于html标签



进入爬虫项目
   item.py       主要设置爬取的目标
   piplines.py   主要用来后续处理
   settings.py   主要用来设置对应的配置
 spiders文件夹下写爬虫文件
   假设spider.py  

 在爬虫文件夹运行爬虫
    scrapy  crawl  pacong(爬虫名)  --nolog   (不需要日志)


 自动爬虫实现的两种思路
   1.通过for循环一直爬下去
   2.初始网址->获得初始网址中的所有的连接->依次的去爬所有的连接->依次爬网页中的所有的链接  (搜索引擎的爬虫通常以这种方式)   

 项目下创建一个普通爬虫文件
    scrapy  genspider  -t basic qiushibaike  qiushibaike.com     (以糗事百科网站为例)


 创建一个自动爬虫  
    新建爬虫项目     scrapy  startproject  qsauto
    创建爬虫文件  (先查看有哪些模板)   scrapy  genspider -l


 自动模拟登录爬虫(以豆瓣为例)
  1. scrapy startproject douban
  2. 进入项目文件夹  cd douban  创建爬虫文件
     scrapy  genspider -t  basic  db  douban.com 
  3. 去编写爬虫文件脚本
  4.运行爬虫   scrapy  crawl  db  (爬虫脚本名为db)


  验证码的处理方式有
     1.图片识别
     2.通过打码接口
     3.采用半自动的方式:假如发现验证码,把验证码的图片下载到本地,然后再手动输入 ,需要配合使用urllib模块

 pip install scrapy==1.1.0rc3   scrapy 常用版本


 setting.py文件中加入代码
 USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'



 分布式爬虫的构件思路:
   用到三个模块: scrapy   scrapy-redis   redis
   注:虽然redis官方说法不支持windows,但是在github上还是有改过的windows版本



socket
服务端:
   1.server = socket.socket(AF.INET,sock.SOCK_SREAM)
           AF.INET      IPV4
           AF.INET6  
   2.server.bind(localhost,9999)
   3.server.listen()
   4.conn,addr = server.accept()    开始阻塞
   5   while  true:






 socket传递数据,先把数据传递到缓冲区
   1:数据大小不到缓冲区的容量大小



   decode() 转化成str  
   encode() 转化成byte
   a = '啊'
   len(a)=1   
   len(a.encode()) = 3 



   int()
   str()

socket接收数据原理
    客户端把命令传递给服务端
    服务器端根据接受的命令给客户端返回数据
       (假设返回的数据有2000,而客户端一次接收的数据1000,这里有缓冲区buffer的概念,剩余的1000就放在缓冲区里.下一次再次交互的数据的时候,优先交互缓冲区里的数据,
       然后你的新的数据放入缓冲区,直到缓冲区满了之后,在send出去)   

    解决问题的方法:服务端给客户端发数据之前,先计算一下数据大小,把数据大小同时发给客户端
                    循环发送  


 linux环境下粘包(python2.7在windows上也会存在这样的情况,python3如果测试次数很多,也会出现这样的问题) 
          缓冲区会把两次send操作当成一次发给客户端,导致数据黏在一起                     
             










正则匹配  任意值  (.+?)





线程      操作系统对硬件操作的一堆指令的集合,是最先的调度单位
          线程被包含在进程中 
          

进程      qq需要以一个整体的形式暴露给操作系统管理,里面包含对各种
          资源(内存,网卡,声卡,cpu)的调用,内存的管理,网络接口的调用等
          对各种资源管理的集合 
          进程要操作cpu进行运算,必须要创建一个进程  
          进程至少包含一个线程  

区别:线程共享内存空间,进程是独立的(qq不能访问word)
     同一个进程的线程之间可以直接交流,两个进程向通信,必须有一个中间进程做代理   
     创建线程很简单,创建新进程需要对其父进程进行克隆
     一个线程可以控制操作同一个进程里的其他线程,但是进程只能操作子进程





     问题:启动一个进程快,还是启动一个线程快?   线程快
          现有线程,再有进程
          启动进程:相当于修一间屋子
          启动线程:相当于拉一个人进来      

python的线程其实调用c语言的原生线程
python sleep的时候是不占用cpu的


递归锁:一个大锁里面还要包含子锁

队列queue:队列数据放内存中  默认先入先出      lifoqueue 后进先出
          作用:程序的解耦,使程序之间实现松耦合
               提高处理效率
               fifo    first in first out
               lifo    last in  first  out


io操作不占用cpu(从硬盘读发数据,重网络上读发数据)(socket server)
计算占用cpu

python的多线程不适合cpu密集操作型的任务,适合io密集型的任务
linux上每一个进程都是父进程启动的


进程池:同一时间有多少进程在cpu上运行



if __name__=='__main__':   意思是如果是主动执行该脚本,则执行if下面的代码
                                 如果是该脚本作为模块被其他基本调用,那么if下面的代码就不执行了

协程:用户态轻量级的线程,微线程
     用户自己控制的,cpu更本都不知道他的存在
协程的好处:  
    无需线程上下文切换的开销,所以可以这样理解,通过协程实现的多并发,其 实是不同函数之间的来回切换
    不涉及cpu的切换
    无需操作锁(程序在串型的状态下执行的)
    高并发,高扩展性,低成本

缺点:无法利用多核的资源,


RabbitMQ:消息队列     erlang语言开发的



redis在linux系统安装目录下三个文件
redis.conf里看daemonoize no/yes   表示在后台/前台启动  一般我们写yes,使redis服务在前台启动
                                  ./redis-server redis.conf
             然后启动redis客户端  ./redis-cli 
redis默认有16个db,0~15

keys *   查看所有的键
del 键名  删除redis
get 键名  查看redis







集合数据的一个特点:无序             


本机linux虚拟机上进入mysql(首先看mysql服务有没有启动)    ./mysql -uroot -p123 

ps -A | grep mysql  查看mysql服务是否启动起来
mysql授权:   grant  all privileges on *.* to 'root'@'%' identified  by  '123';
             flush privileges;
    %:表示允许任意ip都能远程访问,可以写具体的ip

windows上的navcat连接不上虚拟机上的mysql
    GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123' WITH GRANT OPTION;   //123指root设置的密码
    flush privileges;
navicat 远程连接linux上的mysql服务
连接属性  (常规)主机名或ip :localhost    (SSH)主机名或ip:linux服务器名字 




alt+p设置xshell字体



在本地新建一个分支： git branch Branch1
切换到你的新分支: git checkout Branch1
将新分支发布在github上： git push origin Branch1
在本地删除一个分支： git branch -d Branch1
在github远程端删除一个分支： git push origin :Branch1   (分支名前的冒号代表删除)

//需要手工推代码才会用到
git remote add origin https://github.com/gujie1216933842/mysite.git
git push -u origin master        把本地代码推到远端主分支
//需要手工推代码才会用到

linux上初始化git


github使用流程总结:
1.登录github网站,新建仓库(注:分支在本地建好之后推到远程仓库去,在网站上我没有找到新建仓库的地方)
2.用sourcetree克隆功能把远程仓库拉到本地(windwos),推荐使用(假如未安装sourcetree,打开gitbash 输入命令 git clone  https://github.com/gujie1216933842/mysite.git)
3.在windows本地仓库新建一个文件,推到远程仓库,激活主干master,推荐使用sorcetree,如未安装sorcetree,gitbash中输入命令git remote add origin https://github.com/gujie1216933842/mysite.git
4.在linux本地新建仓库,只能使用命令git clone  https://github.com/gujie1216933842/mysite.git,克隆完成,激活主干 git checkout master
5.ok



注:git不识别空文件夹,暂为处理(解决方法:可是暂时在空文件夹下增加一个空文件)

git clone  https://github.com/gujie1216933842/mysite.git
切分支  git checkout -b 分支名   
linux上 从远程github上拉代码 
git pull origin python2017


 git 忽略 Pycharm 中的 workspace.xml 文件
 .gitignore 中要写上 workspace.xml
 如果已经不幸之前commit workspace.xml 的话，必须执行以下命令
git rm --cached .idea/workspace.xml

git rm -r --cached .
git add .
git commit -m 'update .gitignore'




git 在本地新建分支并推送到远端
git branch feature/180310_data
git checkout feature/180310_data
git push origin feature/180310_data   
分以上三步



    
postman     google的账号  gujie1216933842  密码86917307x   邮箱1216933842@qq.com



  


python中==和is
    ==判断两个变量是否相等
    is  判断两个变量是否指向了同一个内存地址

深拷贝和浅拷贝
     b = a  赋值,变量引用 
     c = copy.copy(a)       浅拷贝
     d = copy.deepcopy(a)    深拷贝
     
    深浅拷贝的区别: 假如b对象中仍然包含对象,那么浅拷贝在里面对象那一层就不会重新开辟内存地址了
                    而深拷贝会继续开辟新的内存地址
    如  a = [1,2,['a']]
        a.append(3)
        a[3].append('b')
        程序执行结果: b = [1,2,['a','b'],3]
                      c = [1,2,['a','b']]
                      d = [1,2,['a']]    


func(*args,**kwargs)   
     *args : 参数一元组的形式传递进去
     **kwargs:参数以字典的形式传递进去


打印:  print('----test-a=%d,b=%d'%(1,2))   


globals()  展示python的所有全局变量
locals()   展示python的所有局部变量

python是动态语言,可以再运行的过程中给她修改代码,比如将变量重整形改为函数类型
以c语言/c++为代表的是静态语言,特点是运行之前先编译 



什么是生成器,迭代器,闭包,装饰器

生成器:
生成器的第一钟方式:  a = (i*2 for i in range(1000))
生成器的第二种方式:  函数里有yield,然后去调用函数




python垃圾回收机制
   引用计数
   隔代回收
   查看引用计数


内建属性
    

集合set   一般情况下很少用到  
          作用: 列表去重  返回集合类型
                可以取交集,并集,差集 


python开启一个简单的服务器
     python -m http.server 8888



linux复制文件夹    cp -r 01part/. 02part/
linux文件夹重命名  mv  ab  abc
进程和程序的区别:代码是同一份代码,数据是各自的数据 


创建进程的几种方式
 1. ret = os.fork()  知道创建过程,了解就行
 2. p1 = Process(target = xxx)
    p1.start()
    子进程做事情,主进程也做事情 

 3. 进程池    #主进程一样用来等待,真正的任务都在子进程中执行    
              #一般都是子进程在做事情
    apply_async  非阻塞的方式


进程间通信--queue



线程也是用来实现多任务的工具
线程 thread

进程是     的单位,对应的是操作系统对应的每一个程序
线程是     的单位,对应的是每一个程序同时执行


只要一个程序向下执行,就一定会有一个线程

同步,异步指两个线程挨个执行



    

创建数据库以天天生鲜为例子
  create  database  tiantian  charset=utf8




服务器返回的状态码
  405   请求方式非法(如服务器只有post的请求或者是只有get的请求方式)



tornado基于epoll,epoll工作原理
  epoll是linux操作系统提供的,epoll的运行离不开操作系统,
  epoll来的时候,服务器开启socket,客户端来了请求之后
  epoll首先提供一个监听的socket端口
  epoll当成一个管理员,看哪一个socket传递消息过来,我就操作哪一个socket,其他我就不用管了
  (过程中,不断的询问epoll)

tornado的操作保证没有阻塞的情况,一旦有阻塞,会导致整个tornado的操作都会停下来
tornado整个过程都是单线程


tornado自定义框架结构
   ihome_tornado
       handlers
       html
       libs          引入的别人的类库
       logs
       statics
       utils 


tornado连接数据库的模块torndb暂时还没有支持python3
体现在torndb.py文件中import  mysqlDB   需要改成  import pymysql  pymysql.install_as_MySQLdb()









能用http传递参数的几种方式
1.查询字符串参数(两种&和/) get
2.请求体   post
3.cookie
4.请求头  header






linux查看进程
  ps aux
    





 python中os模块和sys模块有什么区别
 os:操作系统
 sys:指python解析器

os.path    系统的路径
sys.path   调用python解析器时的路径



tornado项目总结
注册页面验证码在服务端生产   验证码生成后存储在redis中

注册页面首次加载,需要加载一次ajax请求验证码的接口

先写验证码的接口代码(python,只写过php^-^)
a.生成验证码(文本,图片)
b.验证码的文本信息存在redis中(redis设置有效期)
c.返回图片(返回的图片资源二进制数据)传给前端




python3中的图像处理模块  Pillow


tornado连接mysql用到的模块torndb.py    由于兼容性较差,实际项目中要做一些修改(py3.5)
line139   itertools.izip(column_names, row) => zip(column_names, row)    (py2与py3的语法差异)
line70    connect_timeout=none  => connect_timeout=10





python3中in range的详细用法
>>> range(1,5) #代表从1到5(不包含5)
[1, 2, 3, 4]
>>> range(1,5,2) #代表从1到5，间隔2(不包含5)
[1, 3]
>>> range(5) #代表从0到5(不包含5)
[0, 1, 2, 3, 4]  


session中三种数据形式


python3 中json模块 dumps()和loads()
dumps()   把python中的dict list 等对象序列化成字符串
loads()   把json字符串反序列化成  python对象(dict)







nginx反向代理的理解:  客户端 -----  nginx服务器 -------   业务服务器(django/tornado服务器)
           反向代理:  用户的请求由nginx服务器转发给业务服务器
                      业务服务器处理完转给nginx,nginx再转给客户端  
                      对于用户而言,知道到nginx的ip和端口,不知道具体处理业务的业务服务器
                      (nginx只是一个代理机构)
           负载均衡:  对业务服务器进行引流

           正向代理:  







python装饰器:

 




linux系统下安装python3
1.  tar -xvf  Python-3.5.2.tar
2.  ./configure --prefix=/usr/local/python3
3.  make && make install
    出错: Ignoring ensurepip failure: pip 8.1.1 requires SSL/TLS
    解决: yum install openssl-devel

4.  给pyhton3 设置环境变量
    vi /etc/profile
    在文件最下面添加   export PATH="$PATH:/NEW_PATH"
    




七牛云使用
https://portal.qiniu.com/bucket/ihome/resource后台登录
内容管理  查看外链默认域名 :   上传图片之后,默认的图片url前缀
页面有手动上传图片的操作,其实相当于微信传图中转,七牛提供了python上传图片的接口,把python代码接入到ihome项目中

七牛  开发者中心 https://developer.qiniu.com/kodo/sdk/1242/python  中有相关的文档







tornadb中没有提供事务操作
事务:一组操作要么都成功,要么都失败


json.loads() 把json格式变成python中的数据格式
json.dumps() 把python中的数据格式变成json数据格式 







scrapy中的文件
pipelines.py    爬虫后续处理的文件    比如讲数据写入数据库中(process_item()方法)
setting.py    pipelines默认是不开启的    开启,修改一下默认方法
爬虫文件中 (以jdmobile.py为例)  
          start_urls = (
                  "https://item.jd.com/11334636448.html",
              )
          网址后一定要加逗号,坑坑坑




scrapy中的xpath表达式
/   从整个html页面顶端向下寻找某一个标签
/div/span   从第一个div下第一个span内容   (注意,要爬取span下的所有文本内容,span后面不能加/,否则会报xpath语法错误)
/html/head     所有head标签和head标签下的内容   <title id = "ddd">啊啊啊啊啊</title>
text()  提取文本信息
@      提取属性信息

/html/head/text()     最终取到的是"啊啊啊啊啊"

/html/head/@id     最终取到的是"ddd"


//div   寻找整个页面的所有的div标签
//div[@class="jifen"]   
//div[@class="jifen"]/a/@href    寻找所有属性是div下第一个a标签的href属性




linux查看服务命令
Linux下如何查看哪些进程占用的CPU内存资源最多
linux下获取占用CPU资源最多的10个进程，可以使用如下命令组合：

ps aux|head -1;ps aux|grep -v PID|sort -rn -k +3|head



linux下获取占用内存资源最多的10个进程，可以使用如下命令组合：

ps aux|head -1;ps aux|grep -v PID|sort -rn -k +4|head

    




命令组合解析（针对CPU的，MEN也同样道理）：

ps aux|head -1;ps aux|grep -v PID|sort -rn -k +3|head



该命令组合实际上是下面两句命令：

ps aux|head -1

ps aux|grep -v PID|sort -rn -k +3|head

 

 

可以使用一下命令查使用内存最多的10个进程

查看占用cpu最高的进程

ps aux|head -1;ps aux|grep -v PID|sort -rn -k +3|head

或者top （然后按下M，注意这里是大写）

查看占用内存最高的进程

ps aux|head -1;ps aux|grep -v PID|sort -rn -k +4|head

或者top （然后按下P，注意这里是大写）

该命令组合实际上是下面两句命令：

ps aux|head -1
ps aux|grep -v PID|sort -rn -k +3|head

其中第一句主要是为了获取标题（USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND）。
接下来的grep -v PID是将ps aux命令得到的标题去掉，即grep不包含PID这三个字母组合的行，再将其中结果使用sort排序。
sort -rn -k +3该命令中的-rn的r表示是结果倒序排列，n为以数值大小排序，而-k +3则是针对第3列的内容进行排序，再使用head命令获取默认前10行数据。(其中的|表示管道操作)

补充:内容解释

PID：进程的ID
USER：进程所有者
PR：进程的优先级别，越小越优先被执行
NInice：值
VIRT：进程占用的虚拟内存
RES：进程占用的物理内存
SHR：进程使用的共享内存
S：进程的状态。S表示休眠，R表示正在运行，Z表示僵死状态，N表示该进程优先值为负数
%CPU：进程占用CPU的使用率
%MEM：进程使用的物理内存和总内存的百分比
TIME+：该进程启动后占用的总的CPU时间，即占用CPU使用时间的累加值。
COMMAND：进程启动命令名称

 

 

 

、可以使用以下命令查使用内存最多的K个进程

方法1：

ps -aux | sort -k4nr | head -K
如果是10个进程，K=10，如果是最高的三个，K=3

说明：ps -aux中（a指代all——所有的进程，u指代userid——执行该进程的用户id，x指代显示所有程序，不以终端机来区分）

        ps -aux的输出格式如下：

USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.0  19352  1308 ?        Ss   Jul29   0:00 /sbin/init
root         2  0.0  0.0      0     0 ?        S    Jul29   0:00 [kthreadd]
root         3  0.0  0.0      0     0 ?        S    Jul29   0:11 [migration/0]
     sort -k4nr中（k代表从第几个位置开始，后面的数字4即是其开始位置，结束位置如果没有，则默认到最后；n指代numberic sort，根据其数值排序；r指代reverse，这里是指反向比较结果，输出时默认从小到大，反向后从大到小。）。本例中，可以看到%MEM在第4个位置，根据%MEM的数值进行由大到小的排序。

     head -K（K指代行数，即输出前几位的结果）

     |为管道符号，将查询出的结果导到下面的命令中进行下一步的操作。

方法2：top （然后按下M，注意大写）

二、可以使用下面命令查使用CPU最多的K个进程

方法1：

ps -aux | sort -k3nr | head -K
方法2：top （然后按下P，注意大写



linux下查看crontab的定时跑批任务
crontab -l





使用django后台管理
django manage.py createsuperuser
启动django服务
访问:localhost:8000/admin
如果后台想要变成中文
在setting.py中做修改   #1.8版本之后   中文'zh-hans'  英文 'en-us'
时区设置    TIME_ZONE = 'Asia/Shanghai'

如果要在admin中查看前台的模型类,需要在应用下的admin.py文件中注册
如果有编码问题  ,需要在代码中  str.encode(utf8)


django设计视图
需要配置路由
在项目配置urls.py引入应用的urls.py(此文件需要在应用目录下新建,并非应用自动生成)
在项目的setting.py文件内需要配置模板文件的路径,参考tiantian项目
具体参考test1应用

模板传递数据
return render(request,'test1/index1.html',data)  //data是字段格式{}

django模型中定义字段的类型都在
django.db.models中,可以自行去寻找

django模型类的管理器Manager
1.管理器是django模型进行数据库查询操作的接口,django应用的每个模型拥有至少一个管理器(model.py)
2.自定义管理器主要分为两种情况
  情况一:向管理器类中添加额外的方法:
  情况二:修改管理器返回原始查询集:举例:重写get_queryset()方法


模型类和模型管理器区分
模型类:是我们自己定义的一个对象(类),这个类要和数据库中的表要相对应,将来数据中的每一条记录数据,就是一个模型类对象

模型管理器 :  模型类和数据库中的数据怎么映射起来,就是通过模型类管理器完成的映射

所以我们定义的顺序: 先定义模型类,然后再定义管理器类,再把定义好的管理器类作为模型类的一个属性


如果数据库表的字段修改了,或者是字段有删减
需要重新执行迁移步骤
如果此时迁移过程总是报错,可以先把自动生成的迁移文件删除,在执行迁移操作


知识点:查询
返回查询集的方法,称为过滤器
返回集合
all()  获取所有的
filter(属性__比较符=值)   筛选  =
filter(heroinfo__hcontent__contains='八')   表示heroinfo表中的hcontent字段中包含'八'的记录
fliter().fliter() 等价 fliter(,) 逻辑与的关系
逻辑或的关系
fliter(Q()|Q())

exclude()  获取不满足条件的数据 !=
order_by()  排序
values()    把模型类对象转化成字典,最终以合成列表返回,不是对象了,可直接向客户端返回json数据,一步到位

返回一条
get()
count()
first()  可写条件
last()   可写条件
exists()  是否满足条件

惰性执行(数据:迭代,序列化,if后)



django模板语言
1.变量
2.标签{%代码块%}
3.过滤器
4.注释{#代码或者html#}

django  表单提交
在form代码内部加  {% csrf_token %}



生成器
定义:如果列表元素可以按照某种算法推算出来,那么我们就不需要创建完整的list,节省大量的空间,在python中,这种以便循环一边计算的机制,称之为生成器,generator

补充理解1: 生成器是一种特殊的程序,可以被用作控制循环的迭代行为,python中生成器是迭代的一种,使用yield返回值函数,每次调用yield会暂停,而可以使用next()函数和send()函数回复生成器

补充理解2:生成器类似于返回值为数组的一个函数,这个函数可以接受参数,可以被调用,但是,不同于一般的函数会一次性返回包括所有数组的数组,生成器已成只能生成一个值,这样消耗的内存数量将大大减少,而且允许调用函数可以很快的处理前几个返回值,因此生成器看起来像是一个函数,但是表现的却像是迭代器


迭代器:是访问集合元素的一种方式,迭代器对象从集合的第一个元素开始访问,知道所有的元素被访问结束,是不是觉得跟for循环很像,但是迭代器有几个特性需要记住
a.访问者不需要关心迭代器内部结构,只需不断执行next()方法获取下一个内容
b.不能随机访问集合中的某一个值,只能从头到尾顺序读取
c.访问到一半是不能回退,不能访问之前的值
d.适合遍历很大的数据集合,节省内存,提升速度

加深理解:linux中的vi和cat命令.cat就相当于利用的迭代器的思想,如果一个文件大于1g,cat命令会从第一个值依次往下输出,不会很卡,而vi命令,会把该文件所有内容放到内存中

缺点:迭代器不能取集合中间某一个值,前一个值读取完即回收内存,所以无法重复读取
     由于迭代器的特性，我们只能顺序依次进行取值，不能像list那样可以取集合中的任意值，在这里三个值都取出后如果再执行
     a.__next__()，则会报错：“已停止迭代”   异常StopIteration


理解生成器和迭代器的区别:
生成器是用函数中yield语句来创建的。迭代器的创建首先跟函数无关,可以用iter([1,2])方法来创建
生成器是一种迭代器，是一种特殊的函数
每个生成器都是一个迭代器，但是反过来不行。通常生成器是通过调用一个或多个yield表达式构成的函数s生成的。同时满足迭代器的定义。



理解epoll
epoll是一种工作机制
epoll是依赖于linux操作系统的,是linux操作系统给我们提供的一种工具,所以说epoll的实现离不开linux操作系统
epoll是一个容器,里面可以管理多个socket
服务器新建连接,服务器开启监听,首先要建立一个socket连接,等待客户端的连接
epoll 相当于socket的管理人

tornado.ioloop.IoLoop.current().start()   表示tornado不断的循环去询问epoll是否我需要执行什么
tornado.ioloop.IoLoop类是tornado运行的核心
ioloop作为全局的统筹者,不断与linux层的epoll一问一答
tornado.ioloop模块封装了epoll操作socket的操作的接口




协程:程序之间相互配合运行的一种机制




































